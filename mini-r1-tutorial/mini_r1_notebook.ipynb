{
 "cells": [
  {
   "cell_type": "raw",
   "id": "207299f5-f879-43e1-8087-f4820c0924f1",
   "metadata": {},
   "source": [
    "\n",
    "https://www.philschmid.de/mini-deepseek-r1\n",
    "https://arxiv.org/pdf/2501.12948\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aaceb27d-1ceb-4812-9212-493f896d08d5",
   "metadata": {},
   "source": [
    "If you want to run the notebook, install the following:\n",
    "peft==0.14.0\n",
    "bitsandbytes==0.45.0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "485f99b2-989f-4d30-bb78-e67416ec238e",
   "metadata": {},
   "source": [
    "\n",
    "Reproducing DeepSeek-R1-Zero's Aha Moment\n",
    "Using simplified examples (the Countdown Game)\n",
    "\n",
    "The Countdown game is a numbers puzzle where players use a set of randomly drawn numbers and basic arithmetic operations (+, -, ร, รท) to reach or get as close as possible to a target number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58384c02-557a-4095-875d-8fc1bf345751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face for access to OSS models, datasets, and logging experiments\n",
    "\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "login(token=os.getenv('HUGGINGFACE'), add_to_git_credential=True) # ADD YOUR TOKEN HERE"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b5a33a0-60d4-4412-b66d-0540983eb983",
   "metadata": {},
   "source": [
    "\n",
    "Using\n",
    "Dataset: Countdown-Tasks-3to4\n",
    "Model: Qwen/Qwen2.5-3B-Instruct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2842af0-5bfb-41be-ab19-215faff08d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    " \n",
    "# Load dataset from Hugging Face Hub\n",
    "dataset_id = \"Jiayi-Pan/Countdown-Tasks-3to4\"\n",
    "dataset = load_dataset(dataset_id, split=\"train\")\n",
    "# select a random subset of 50k samples\n",
    "dataset = dataset.shuffle(seed=42).select(range(50000))\n",
    " \n",
    "# Load tokenizer from Hugging Face Hub to format the dataset to our \"r1\" prompt \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
    " \n",
    "# gemerate r1 prompt with a prefix for the model to already start with the thinking process\n",
    "def generate_r1_prompt(numbers, target):\n",
    "    r1_prefix = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer.\"\n",
    "      },\n",
    "      { \n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Using the numbers {numbers}, create an equation that equals {target}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final equation and answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 = 1 </answer>.\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Let me solve this step by step.\\n<think>\"\n",
    "      }]\n",
    "    return {\"prompt\": tokenizer.apply_chat_template(r1_prefix, tokenize=False, continue_final_message=True), \"target\": target}\n",
    " \n",
    "# convert our dataset to the r1 prompt\n",
    "dataset = dataset.map(lambda x: generate_r1_prompt(x[\"nums\"], x[\"target\"]))\n",
    " \n",
    "# split the dataset into train and test\n",
    "train_test_split = dataset.train_test_split(test_size=0.1)\n",
    " \n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f72ef-7816-48a3-9273-928bd396603c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "69fd25ba-82d2-4381-83b7-a32846344c6a",
   "metadata": {},
   "source": [
    "Training a model using GRPO:\n",
    "\n",
    "Training is done using TRL's GRPOTrainer in Trainer.\n",
    "Reward is computed using custom rule-based reward models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53c73e87-36f8-4702-97c5-0689d214251a",
   "metadata": {},
   "source": [
    "Formatting reward:\n",
    "\n",
    "Two functions below to:\n",
    "1. Format reward: check if the generated format is correct:\n",
    "    '<think> [thinking] </think><answer> [answer] </answer>'\n",
    "2. Extract the response from <answer> tag and evaluate it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5fd012-9ee0-4f3d-8c67-359fc1bd829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "def format_reward_func(completions, target, **kwargs):\n",
    "    \"\"\"\n",
    "    Format: <think>...</think><answer>...</answer>\n",
    "    Args:\n",
    "        completions (list[str]): Generated outputs\n",
    "        target (list[str]): Expected answers\n",
    "      \n",
    "      Returns:\n",
    "          list[float]: Reward scores\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    " \n",
    "    for completion, gt in zip(completions, target):\n",
    " \n",
    "      try:\n",
    "        # add synthetic <think> as its already part of the prompt and prefilled for the assistant to more easily match the regex\n",
    "        completion = \"<think>\" + completion        \n",
    "        # Check if the format is correct\n",
    "        regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n",
    " \n",
    "        match = re.search(regex, completion, re.DOTALL) \n",
    "        # if the format is not correct, reward is 0\n",
    "        if match is None or len(match.groups()) != 2:\n",
    "            rewards.append(0.0)\n",
    "        else:\n",
    "            rewards.append(1.0)\n",
    "      except Exception:\n",
    "        rewards.append(0.0)\n",
    "    return rewards\n",
    " \n",
    "def equation_reward_func(completions, target, nums, **kwargs):\n",
    "    \"\"\"\n",
    "    Evaluates completions based on Mathematical correctness of the answer\n",
    " \n",
    "    Args:\n",
    "        completions (list[str]): Generated outputs\n",
    "        target (list[str]): Expected answers\n",
    "        nums (list[str]): Available numbers\n",
    "    \n",
    "    Returns:\n",
    "        list[float]: Reward scores\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion, gt, numbers in zip(completions, target, nums):\n",
    "      try:\n",
    "        # add synthetic <think> as its already part of the prompt and prefilled for the assistant to more easily match the regex\n",
    "        completion = \"<think>\" + completion\n",
    "        # Check if the format is correct\n",
    "        match = re.search(r\"<answer>(.*?)<\\/answer>\", completion)\n",
    "        if match is None:\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "        # Extract the \"answer\" part from the completion\n",
    "        equation = match.group(1).strip()\n",
    "        # Extract all numbers from the equation\n",
    "        used_numbers = [int(n) for n in re.findall(r'\\d+', equation)]\n",
    "        \n",
    "        # Check if all numbers are used exactly once\n",
    "        if sorted(used_numbers) != sorted(numbers):\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "        # Define a regex pattern that only allows numbers, operators, parentheses, and whitespace\n",
    "        allowed_pattern = r'^[\\d+\\-*/().\\s]+$'\n",
    "        if not re.match(allowed_pattern, equation):\n",
    "           rewards.append(0.0)\n",
    "           continue\n",
    "        \n",
    "        # Evaluate the equation with restricted globals and locals\n",
    "        result = eval(equation, {\"__builtins__\": None}, {})\n",
    "        # Check if the equation is correct and matches the ground truth\n",
    "        if abs(float(result) - float(gt)) < 1e-5:\n",
    "            rewards.append(1.0)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "      except Exception:\n",
    "            # If evaluation fails, reward is 0\n",
    "            rewards.append(0.0) \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "raw",
   "id": "938ced11-6471-4a1b-9e91-a7e9b908e3b8",
   "metadata": {},
   "source": [
    "\n",
    "Checking reward functions with sample inputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9374a4-8c18-4c65-9586-bff45ec5ca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_sample_1 = \"\"\"We need to find an equation using the numbers 19, 36, 55, and 7\n",
    "exactly once, with basic arithmetic operations, that equals 65. One possible\n",
    "combination is 55 + 36 - 19 + 7... </think>\n",
    "<answer> 55 + 36 - 7 - 19 </answer>\"\"\"\n",
    " \n",
    "correct_sample_2 = \"\"\" ... </think>\n",
    "<answer> 55 + 36 - 7 - 19 </answer>\"\"\"\n",
    " \n",
    "wrong_format = \"\"\"User: Using the numbers [19, 36, 55, 7], create an equation that equals 65.\"\"\"\n",
    " \n",
    "wrong_format_2 = \"\"\"To find the equation that equals 79 using the numbers 95, 78, 6, 88, I'll start by adding 88 and 95:                      \n",
    "95 + 88 = 183                                                                                                              \n",
    "Now, let's subtract 104 from 183 to get 79:\n",
    "183 - 104 = 79\n",
    "<think> 183 - 104 = 79 </think><think> 183 - 104 = 79 </think><answer> 183 - 104 = 79 </answer>\"\"\"\n",
    " \n",
    "wrong_result = \"\"\" ... </think>\n",
    "<answer> 55 + 36 - 7 - 18 </answer>\"\"\"\n",
    " \n",
    " \n",
    "test_rewards = format_reward_func(completions=[correct_sample_1, correct_sample_2, wrong_format, wrong_format_2, wrong_result], target=[\"65\", \"65\", \"65\", \"65\", \"65\"], nums=[[19, 36, 55, 7]] * 5)\n",
    "assert test_rewards == [1.0, 1.0, 0.0, 0.0, 1.0], \"Reward function is not working\"\n",
    "test_rewards = equation_reward_func(completions=[correct_sample_1, correct_sample_2, wrong_format, wrong_format_2, wrong_result], target=[\"65\", \"65\", \"65\", \"65\", \"65\"], nums=[[19, 36, 55, 7]] * 5)\n",
    "assert test_rewards == [1.0, 1.0, 0.0, 0.0, 0.0], \"Reward function is not working\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd493f07-d33c-422a-9ce7-61a21c392e54",
   "metadata": {},
   "source": [
    "\n",
    "Setting up TRL training.\n",
    "Below setup will take more than 20 minutes on L4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7448e0e3-d1ac-492e-abbf-85a920052221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer, get_peft_config, ModelConfig\n",
    " \n",
    "# our model we are going to use as policy \n",
    "model_config = ModelConfig(\n",
    "    model_name_or_path=\"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    use_peft=True,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    " \n",
    "# Hyperparameters\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"qwen-r1-aha-moment\",\n",
    "    learning_rate=5e-7,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10,\n",
    "    max_steps=100,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    bf16=True,\n",
    "    # GRPO specific parameters\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=1024, # max length of the generated output for our solution\n",
    "    num_generations=2,\n",
    "    beta=0.001,\n",
    "    \n",
    ")\n",
    "trainer = GRPOTrainer(\n",
    "    model=model_config.model_name_or_path,\n",
    "    reward_funcs=[format_reward_func, equation_reward_func],\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=get_peft_config(model_config),\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70a122cb-999a-4bbf-8d8b-40960f4c3037",
   "metadata": {},
   "source": [
    "\n",
    "Example of Distributed Training:\n",
    "\n",
    "Script:\n",
    "https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/scripts/run_r1_grpo.py\n",
    "\n",
    "Command:\n",
    "accelerate launch --num_processes 3 --config_file configs/accelerate_configs/deepspeed_zero3.yaml scripts/run_r1_grpo.py --config receipes/grpo-qwen-2.5-3b-deepseek-r1-countdown.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288f2bc4-79de-4627-a8a7-f1cea57edef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
